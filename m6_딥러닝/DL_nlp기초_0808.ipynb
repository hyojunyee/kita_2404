{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["!pip install keras\n","!pip install tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZGumm5p_UYi","executionInfo":{"status":"ok","timestamp":1723087030419,"user_tz":-540,"elapsed":3942,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"8e00ecc6-3aa7-4616-d968-89b93f7e3ac1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.65.4)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}]},{"cell_type":"markdown","source":["## 텍스트의 토큰화\n","**자연어 처리(NLP)에서 토큰화(Tokenization)**\n","- NLP: Natural Language Processing\n","- 텍스트 분석, 언어 번역, 감정 분석 등 다양한 NLP 작업을 위해 텍스트를 준비하는 기본 단계입니다.\n","- 토큰화는 텍스트를 단어, 문구, 기호 또는 기타 의미 있는 요소(토큰)로 분해하는 작업을 포함합니다. 이 과정에서 생성된 토큰은 추가 처리 및 분석을 위한 기본 구성 요소가 됩니다.\n","\n","토큰화의 목적\n","- 토큰화의 주요 목적은 텍스트 데이터를 단순화하여 알고리즘이 이해하고 처리할 수 있도록 관리하기 쉽게 만드는 것입니다. 이를 통해 텍스트의 복잡성을 줄이고 일관성을 유지함으로써, 다양한 NLP 작업에서 효율적인 분석과 처리가 가능해집니다.\n","\n","토큰화의 유형\n","- 토큰화는 다양한 수준에서 수행될 수 있으며, 각 유형은 특정 NLP 작업의 요구 사항에 따라 선택됩니다:\n","  - 단어 토큰화 (Word Tokenization):\n","    텍스트를 개별 단어로 분해합니다.\n","    예: \"ChatGPT is amazing!\" → [\"ChatGPT\", \"is\", \"amazing\", \"!\"]\n","  - 문장 토큰화 (Sentence Tokenization):\n","    텍스트를 개별 문장으로 분해합니다.\n","    예: \"Hello world. How are you?\" → [\"Hello world.\", \"How are you?\"]\n","  - 하위 단어 토큰화 (Subword Tokenization):\n","    단어를 더 작은 의미 단위로 분해합니다. 주로 BPE(Byte Pair Encoding)나 WordPiece 알고리즘을 사용합니다.\n","    예: \"unhappiness\" → [\"un\", \"hap\", \"pi\", \"ness\"]\n","  - 문자 토큰화 (Character Tokenization):\n","    텍스트를 개별 문자로 분해합니다.\n","    예: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n","\n","토큰화의 과정\n","- 토큰화 과정은 일반적으로 다음 단계로 구성됩니다:\n","  - 텍스트 정규화 (Text Normalization):\n","    모든 텍스트를 소문자로 변환하여 일관성을 유지합니다.\n","    불필요한 구두점과 공백을 제거합니다.\n","    예: \"Hello, World!\" → \"hello world\"\n","  - 구분자 사용 (Delimiter-based Tokenization):\n","    공백이나 구두점을 기준으로 텍스트를 분해합니다.\n","    예: \"hello world\" → [\"hello\", \"world\"]\n","  - 고급 토큰화 기법 (Advanced Tokenization Techniques):\n","    언어의 문법적, 의미적 구조를 고려하여 토큰을 생성합니다.\n","    BPE, WordPiece, SentencePiece 등의 알고리즘을 사용합니다.\n","\n","토큰화의 중요성\n","- 토큰화는 NLP 작업에서 매우 중요한 역할을 합니다. 잘못된 토큰화는 후속 처리와 분석의 정확도에 큰 영향을 미칠 수 있습니다.\n","- 반면, 올바른 토큰화는 텍스트 데이터를 효과적으로 전처리하고 분석할 수 있게 합니다."],"metadata":{"id":"jKDmhSXl8xqo"}},{"cell_type":"markdown","source":["### 고급 토큰화 기법\n","언어의 문법적, 의미적 구조를 고려하여 토큰을 생성하는 것은 텍스트의 의미를 더 잘 보존하고, 더 정확한 분석과 처리를 가능하게 하기 위한 고급 토큰화 기법입니다. 이러한 기법들은 단순히 공백이나 구두점을 기준으로 텍스트를 분해하는 것을 넘어, 단어의 의미와 형태, 문장의 구조 등을 이해하여 더 정교한 토큰을 생성합니다.\n","\n","\n","형태소 분석 (Morphological Analysis):\n","- 단어를 구성하는 최소 의미 단위인 형태소를 분석합니다.\n","- 예를 들어, \"cats\"는 \"cat\"과 복수형 접미사 \"s\"로 분해됩니다.\n","- 형태소 분석기는 단어의 어간과 접사(접두사, 접미사)를 인식하고 분리합니다.\n","\n","어간 추출 (Stemming):\n","- 단어의 어간을 추출하여 형태를 단순화합니다.\n","- 예: \"running\", \"runs\", \"ran\" → \"run\"\n","- 포터 스테머(Porter Stemmer)와 같은 알고리즘이 사용됩니다.\n","\n","어근 추출 (Lemmatization):\n","- 단어의 어근을 추출하여 형태를 표준화합니다. 어간 추출보다 더 정교합니다.\n","- 예: \"running\", \"ran\" → \"run\"\n","- 품사 정보를 사용하여 정확한 어근을 찾아냅니다. 예를 들어, \"better\"는 어근 \"good\"으로 변환됩니다.\n","- WordNetLemmatizer와 같은 도구가 사용됩니다.\n","\n","BPE (Byte Pair Encoding):\n","- 자주 등장하는 바이트 쌍을 병합하여 점진적으로 단어를 분해합니다.\n","- 예: \"lowest\"가 \"l\", \"o\", \"w\", \"e\", \"s\", \"t\"로 분해되고, 자주 등장하는 \"lo\", \"we\"가 결합되어 \"low\", \"est\"로 변환됩니다.\n","- BPE는 신경망 번역 모델과 같은 대규모 언어 모델에서 널리 사용됩니다.\n","\n","WordPiece:\n","- BPE와 유사하지만, 서브워드(subword) 단위로 토큰을 생성합니다.\n","- 예: \"unhappiness\" → [\"un\", \"##happiness\"]\n","- 트랜스포머 모델(BERT 등)에서 사용됩니다.\n","\n","SentencePiece:\n","- 언어에 중립적인 방식으로 텍스트를 서브워드 단위로 분해합니다.\n","- BPE와 유사하지만, 문장을 토큰화하는 과정에서 공백을 고려하지 않음.\n","- 예: \"unhappiness\" → [\"un\", \"ha\", \"ppiness\"]\n","- Google의 T5, ALBERT 모델에서 사용됩니다."],"metadata":{"id":"cwgDVTs293cg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWTkmmBT8WsS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723087112105,"user_tz":-540,"elapsed":330,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"e4661d00-074d-44c6-8543-e389218fa7c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["원문:\n","해보지 않으면 해낼 수 없다.\n","토큰화:\n","['해보지', '않으면', '해낼', '수', '없다']\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Flatten\n","from tensorflow.keras.utils import to_categorical\n","from numpy import array\n","\n","# 케라스의 텍스트 전처리와 관련한 함수중 text_to_sequence 함수를 load\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","# 전처리 할 텍스트를 정함\n","text = '해보지 않으면 해낼 수 없다.'\n","\n","# 해당 텍스트를 토큰화\n","result = text_to_word_sequence(text)\n","print(f'원문:\\n{text}\\n토큰화:\\n{result}')"]},{"cell_type":"markdown","source":["- Tokenizer' 클래스는 텍스트를 정수 시퀀스로 변환하도록 설계\n","- fit_on_texts(docs): 이 메소드는 문장 목록(docs)을 인수로 사용하여 token 개체에서 호출된다. 텍스트 목록을 기반으로 내부 어휘를 업데이트하여 토크나이저가 이러한 텍스트로 작업할 수 있도록 준비한다. 말뭉치의 각 고유 단어에 색인을 할당하고 단어 빈도와 같은 다양한 측정항목을 계산하는 작업이 포함된다.\n","- token.word_counts: 토크나이저를 텍스트에 맞춘 후 word_counts는 키가 입력 텍스트에서 발견된 단어이고 값은 각 단어의 발생 횟수인 OrderedDict를 제공한다. 'OrderedDict'를 사용하면 단어가 텍스트에서 처음 나타나는 순서대로 정렬.\n","- token.document_count: 이 속성은 처리된 총 문서(또는 문장) 수를 표시\n","- token.word_docs: word_counts와 유사한 OrderedDict이지만 단어의 빈도 대신 각 단어가 나타나는 문서 수를 표시\n","- token.word_index: 이 속성은 단어를 고유하게 할당된 정수에 매핑하는 OrderedDict를 제공. 모델에는 숫자 입력이 필요하므로 이는 기계 학습 모델의 텍스트를 벡터화하는 데 필요"],"metadata":{"id":"9aXCKgiVAStq"}},{"cell_type":"code","source":["# 단어 빈도수 세기\n","# 전처리하려는 세게의 문장을 정함.\n","docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n","       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n","        '토큰화 한 결과는 딥러닝에서 사용할 수 있습니다.']\n","\n","# 토큰화 함수를 이용해 전처리 하는 과정\n","token = Tokenizer()             # 토큰화 함수 지정\n","token.fit_on_texts(docs)        # 토큰화 함수에 문장 적용\n","\n","# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n","# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클레스를 사용\n","print(f'단어 카운트:\\n{token.word_counts}')\n","print(f'\\n문장 카운트:\\n{token.document_count}')\n","print(f'\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n{token.word_docs}')\n","# token.word_index의 출력 순서는 제공된 텍스트 코퍼스(말뭉치)의 각 단어의 빈도에 따라 가장 빈번한 단어부터 가장 빈도가 낮은 단어까지 결정\n","# 동일한 빈도의 경우는 먼저 등장한 단어가 더 낮은 인덱스를 할당\n","print(f'\\n각 단어에 매겨진 인덱스 값:\\n{token.word_index}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gGZi9K1ATEY","executionInfo":{"status":"ok","timestamp":1723087647727,"user_tz":-540,"elapsed":324,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"884a9715-c313-4891-8983-895a272bc373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 카운트:\n","OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n","\n","문장 카운트:\n","3\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n","defaultdict(<class 'int'>, {'토큰화': 3, '텍스트의': 2, '각': 1, '단어를': 1, '합니다': 1, '먼저': 1, '나누어': 1, '해야': 1, '단어로': 1, '인식됩니다': 1, '딥러닝에서': 2, '수': 1, '한': 1, '결과는': 1, '있습니다': 1, '사용할': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n","{'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용할': 14, '수': 15, '있습니다': 16}\n"]}]},{"cell_type":"markdown","source":["Q. 주어진 docs를 토큰화 해서 아래 사항을 수행하세요"],"metadata":{"id":"7scfD1hJC6cJ"}},{"cell_type":"code","source":["docs = ['검찰이 제시한 혐의 사실 전부를 재판부가 무죄로 판단하면서 이 회장은 검찰 기소 이후 3년 5개월여 만에 시름을 덜게 됐다.',\n","'검찰 항소로 2심 재판이 진행될 것이란 전망이 나오지만,']\n","\n","token = Tokenizer()\n","token.fit_on_texts(docs)\n","\n","print(f'단어 카운트:\\n{token.word_counts}')\n","print(f'\\n문장 카운트:\\n{token.document_count}')\n","print(f'\\n각 단어가 몇 개의 문장에 포함되어 있는가:\\n{token.word_docs}')\n","print(f'\\n각 단어에 매겨진 인덱스 값:\\n{token.word_index}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8psyRwHlC-JU","executionInfo":{"status":"ok","timestamp":1723088054106,"user_tz":-540,"elapsed":332,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"8462330c-450c-49c2-81ad-3d926a9a435b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["단어 카운트:\n","OrderedDict([('검찰이', 1), ('제시한', 1), ('혐의', 1), ('사실', 1), ('전부를', 1), ('재판부가', 1), ('무죄로', 1), ('판단하면서', 1), ('이', 1), ('회장은', 1), ('검찰', 2), ('기소', 1), ('이후', 1), ('3년', 1), ('5개월여', 1), ('만에', 1), ('시름을', 1), ('덜게', 1), ('됐다', 1), ('항소로', 1), ('2심', 1), ('재판이', 1), ('진행될', 1), ('것이란', 1), ('전망이', 1), ('나오지만', 1)])\n","\n","문장 카운트:\n","2\n","\n","각 단어가 몇 개의 문장에 포함되어 있는가:\n","defaultdict(<class 'int'>, {'무죄로': 1, '회장은': 1, '5개월여': 1, '전부를': 1, '사실': 1, '덜게': 1, '시름을': 1, '됐다': 1, '검찰': 2, '기소': 1, '이': 1, '3년': 1, '이후': 1, '판단하면서': 1, '재판부가': 1, '만에': 1, '혐의': 1, '제시한': 1, '검찰이': 1, '항소로': 1, '것이란': 1, '진행될': 1, '2심': 1, '전망이': 1, '나오지만': 1, '재판이': 1})\n","\n","각 단어에 매겨진 인덱스 값:\n","{'검찰': 1, '검찰이': 2, '제시한': 3, '혐의': 4, '사실': 5, '전부를': 6, '재판부가': 7, '무죄로': 8, '판단하면서': 9, '이': 10, '회장은': 11, '기소': 12, '이후': 13, '3년': 14, '5개월여': 15, '만에': 16, '시름을': 17, '덜게': 18, '됐다': 19, '항소로': 20, '2심': 21, '재판이': 22, '진행될': 23, '것이란': 24, '전망이': 25, '나오지만': 26}\n"]}]},{"cell_type":"markdown","source":["Keras의 Tokenizer를 사용하여 텍스트 데이터를 정수 인덱스 시퀀스로 변환한 후, 이를 One-Hot Encoding 형식으로 변환하는 과정은 NLP 모델의 입력 데이터를 준비하는 중요한 단계입니다.\n","\n","Tokenizer를 사용한 텍스트 토큰화\n","\n","word_index:\n","- token.word_index는 각 단어를 고유한 정수 인덱스로 매핑한 딕셔너리입니다. 키는 단어이고, 값은 해당 단어의 인덱스입니다.\n","- 이 딕셔너리의 길이(len(token.word_index))는 말뭉치에 있는 고유 단어의 총 개수를 나타냅니다.\n","\n","word_size:\n","- word_size는 고유 단어의 총 개수에 1을 더한 값입니다. 이는 NLP에서 일반적인 관행으로, \"0\" 인덱스를 포함하기 위해 사용됩니다.\n","- \"0\" 인덱스는 패딩(padding)에 사용되거나, 구현에 따라 알 수 없는 단어를 나타낼 수 있습니다.\n","\n","One-Hot Encoding:\n","- to_categorical 함수는 클래스 벡터(정수 인덱스)를 바이너리 클래스 행렬로 변환합니다.\n","- x는 단어를 나타내는 정수 인덱스의 목록 또는 배열이어야 하며, - to_categorical은 이를 One-Hot Encoding 형식으로 변환합니다.\n","- 각 정수에 대해 해당 인덱스 위치만 1로 설정되고 나머지 위치는 0인 벡터를 생성합니다.\n","\n","num_classes:\n","- num_classes는 총 클래스 수를 지정합니다. 이 경우 어휘 크기(word_size)로 설정되어, One-Hot Encoding에 어휘의 모든 단어에 대한 슬롯과 추가 \"0\" 인덱스가 있는지 확인합니다."],"metadata":{"id":"NaaPU4BiEtqB"}},{"cell_type":"markdown","source":["Embedding 레이어\n","- 자연어 처리나 시퀀스 데이터를 다루는 모델에서 자주 사용되는 레이어로, 입력 데이터를 고정된 크기의 고차원 벡터로 변환해주는 역할을 합니다.\n","\n","주요 기능:\n","  - 단어를 벡터로 변환: Embedding 레이어는 고유한 정수 인덱스로 표현된 단어들을 특정 크기의 벡터로 변환합니다. 이 벡터들은 학습 과정에서 모델이 조정할 수 있는 가중치로 초기화됩니다.\n","  - 차원 축소 및 특성 학습: 높은 차원의 단어를 저차원의 벡터로 변환함으로써 데이터의 차원을 축소하고, 단어 간의 유사성을 학습할 수 있습니다. 예를 들어, 비슷한 의미의 단어들이 유사한 벡터로 매핑되도록 학습됩니다.\n","\n","입력 및 출력:\n","  - 입력: 정수로 인코딩된 시퀀스 데이터. 예를 들어, 단어를 고유한 정수로 인코딩한 시퀀스.\n","  - 출력: 각 입력 정수에 대응하는 고정된 크기의 밀집 벡터."],"metadata":{"id":"IKQXZ49DeMTz"}},{"cell_type":"code","source":["# Keras Tokenizer의 fit_on_text 메소드는 일반적으로 문자열의 리스트를 기대하기 때문에, 단일 문자열을 입력하는 것은 적절치 않음.\n","text = \"오랫동안 꿈꾸는 이는 그 꿈을 닮아간다\"\n","token = Tokenizer()\n","token.fit_on_texts([text])\n","print(token.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZMB72JLEthm","executionInfo":{"status":"ok","timestamp":1723088706785,"user_tz":-540,"elapsed":332,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"64682951-c43c-40d7-9376-832d128ee6ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"]}]},{"cell_type":"code","source":["x = token.texts_to_sequences([text])\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBbl-xrLFQnO","executionInfo":{"status":"ok","timestamp":1723088707117,"user_tz":-540,"elapsed":6,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"5d878ddc-0ee1-40cc-f0d2-262049c280d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5, 6]]\n"]}]},{"cell_type":"code","source":["# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n","# word_size에 1을 추가하는 이유는 Keras의 Tokenizer를 사용할 때 0 인덱스 패딩(padding)을 위해 예약하는 관례 때문\n","word_size = len(token.word_index) + 1\n","x = to_categorical(x, num_classes = word_size)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19P6maJfFVAb","executionInfo":{"status":"ok","timestamp":1723088740978,"user_tz":-540,"elapsed":408,"user":{"displayName":"이지라이프","userId":"16840309482463980739"}},"outputId":"1c5aa52c-82bc-49db-8f30-575e5f041a9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 1.]]]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Input\n","\n","# 입력 시퀀스의 길이를 3이라고 가정\n","input_data = tf.constant(([1,2,3]), dtype=tf.int32)\n","\n","# 임베딩 레이어 정의\n","embedding_layer = Embedding(input_dim=100, output_dim=10)\n","\n","# 텐서로 전달해야 함 (임베딩 레이어 적용)\n","output = embedding_layer(input_data)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"sz4jKCLqfEO3","executionInfo":{"status":"error","timestamp":1723162606289,"user_tz":-540,"elapsed":6,"user":{"displayName":"이쥐","userId":"10532578428319600508"}},"outputId":"7f818f39-cfad-41f6-b6c1-f4304d872899"},"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"unmatched ')' (<ipython-input-1-54d63cf7f923>, line 5)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-54d63cf7f923>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    input_data = tf.constant([1,2,3]), dtype=tf.int32)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"]}]}]}