{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAEDLKOnW5iPY/FS4aJg9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyojunyee/kita_2404/blob/main/m6_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/Task/Task_0806.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1_0806. 다음 사항을 준수하여 Fashion MNINS 데이터셋에 대하여 Sequential 방식으로 모델 생성 및 평가를 수행하세요.\n",
        "- 입력 계층 :layers.InputLayer(input_shape=(28*28,))\n",
        "- 첫 번째 Dense 계층 : 출력 512, activation='relu'\n",
        "- 두 번째 Dense 계층 : 출력 256, activation='relu'\n",
        "- 출력 계층 : 출력 10, activation='softmax'\n",
        "- 모델 컴파일 : optimizer='adam', loss='categorical_crossentropy',         metrics=['accuracy']\n",
        "- 모델 학습 : epochs=10, batch_size=64"
      ],
      "metadata": {
        "id": "58IOEAjnza_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 셋: Fashion MNIST\n",
        "기존의 MNIST 데이터셋과 같은 형식을 가지고 있지만, 손으로 쓴 숫자 대신에 10가지 범주의 패션아이템(예: 티셔츠, 바지, 신발 등)의 이미지로 구성"
      ],
      "metadata": {
        "id": "11a7tJvlzvdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Fashion MNIST 데이터셋 로드\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "EIX3NXC-OIVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0].shape\n",
        "import matplotlib.pyplot as plt\n",
        "image_data = train_images[0]\n",
        "\n",
        "plt.figure(figsize=(1, 1))\n",
        "plt.imshow(image_data.reshape(28, 28), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "Op9090rMNiLC",
        "outputId": "20112034-cd69-4247-c160-8798dd200150"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 100x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAg5xJ55OOhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Fashion MNIST 데이터셋 로드\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# 데이터 전처리\n",
        "# Reshape the images to have a single color channel and normalize pixel values\n",
        "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# 모델 정의\n",
        "model = models.Sequential()\n",
        "\n",
        "# 입력 계층\n",
        "model.add(layers.InputLayer(input_shape=(28 * 28,)))\n",
        "# 첫 번째 Dense 계층\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "# 두 번째 Dense 계층\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "# 출력 계층\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# # 모델 입력계층 및 다른계층을 한번에\n",
        "# model = models.Sequential([\n",
        "#     # 입력계층 및 첫 번째 Dense 계층\n",
        "#     layers.InputLayer(input_shape=(28 * 28,)),\n",
        "#     layers.Dense(512, activation='relu'),\n",
        "#     # 두번째 Dense 계층\n",
        "#     layers.Dense(256, activation='relu'),\n",
        "#     # 세번째 Dense 계층\n",
        "#     layers.Dense(10, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqI0jYPiNC65",
        "outputId": "8f6f8098-f826-419e-95ce-6a0184d18eaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 15ms/step - accuracy: 0.7801 - loss: 0.6125\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.8672 - loss: 0.3642\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8794 - loss: 0.3242\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.8917 - loss: 0.2907\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.8973 - loss: 0.2726\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9011 - loss: 0.2646\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9076 - loss: 0.2480\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step - accuracy: 0.9109 - loss: 0.2318\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 15ms/step - accuracy: 0.9138 - loss: 0.2289\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - accuracy: 0.9195 - loss: 0.2071\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8854 - loss: 0.3408\n",
            "Test accuracy: 0.8885999917984009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# from tensorflow.keras.datasets import fashion_mnist\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# # Fashion MNIST 데이터셋 로드\n",
        "# (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# # 데이터 전처리\n",
        "# # 이미지를 0과 1사이의 값으로 정규화\n",
        "# train_images = train_images / 255\n",
        "# test_images = test_images / 255\n",
        "# # 이미지 데이터를 벡터로 변환 (28x28이미지를 784크기의 벡터로 평탄화)\n",
        "# train_images = train_images.reshape(-1, 28 * 28)\n",
        "# test_images = test_images.reshape(-1, 28 * 28)\n",
        "\n",
        "# # 레이블 원 핫 인코딩\n",
        "# train_labels = to_categorical(train_labels)\n",
        "# test_labels = to_categorical(test_labels)\n",
        "\n",
        "# # 모델 정의\n",
        "# model = models.Sequential()\n",
        "\n",
        "# # 입력 계층\n",
        "# model.add(layers.InputLayer(input_shape=(28 * 28,)))\n",
        "# # 첫 번째 Dense 계층\n",
        "# model.add(layers.Dense(512, activation='relu'))\n",
        "# # 두 번째 Dense 계층\n",
        "# model.add(layers.Dense(256, activation='relu'))\n",
        "# # 출력 계층\n",
        "# model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# # 모델 컴파일\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # 모델 학습\n",
        "# model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "\n",
        "# # 모델 평가\n",
        "# test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "# print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBYnaUUCOE2Q",
        "outputId": "f906c6c3-f3e9-4b92-b1f0-adeacffe84cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - accuracy: 0.7904 - loss: 0.5983\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8680 - loss: 0.3588\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.8834 - loss: 0.3149\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.8929 - loss: 0.2904\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.8959 - loss: 0.2782\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9044 - loss: 0.2560\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9074 - loss: 0.2457\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - accuracy: 0.9102 - loss: 0.2339\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9143 - loss: 0.2239\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9196 - loss: 0.2127\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8846 - loss: 0.3395\n",
            "Test accuracy: 0.8871999979019165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 사용"
      ],
      "metadata": {
        "id": "FVX9QmH7R2rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Fashion MNIST 데이터셋 로드\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# 데이터 전처리\n",
        "# 이미지를 0과 1사이의 값으로 정규화\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# 레이블 원 핫 인코딩\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Reshape the images to have a single color channel and normalize pixel values\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "\n",
        "# 모델 정의(컨볼루션 레이어 추가)\n",
        "model = models.Sequential([\n",
        "    # 컨볼루션 레이어 추가\n",
        "    layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    # Dense 레이어 추가\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=64)\n",
        "\n",
        "# 모델 평가\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVJQQa1HRwyx",
        "outputId": "49fce1f6-8a85-4eaf-f52b-0aee46079b6e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 53ms/step - accuracy: 0.7144 - loss: 0.7940\n",
            "Epoch 2/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.8663 - loss: 0.3719\n",
            "Epoch 3/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 53ms/step - accuracy: 0.8887 - loss: 0.3093\n",
            "Epoch 4/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.8979 - loss: 0.2777\n",
            "Epoch 5/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 53ms/step - accuracy: 0.9084 - loss: 0.2507\n",
            "Epoch 6/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 55ms/step - accuracy: 0.9170 - loss: 0.2271\n",
            "Epoch 7/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 53ms/step - accuracy: 0.9201 - loss: 0.2143\n",
            "Epoch 8/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 51ms/step - accuracy: 0.9274 - loss: 0.1937\n",
            "Epoch 9/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 53ms/step - accuracy: 0.9348 - loss: 0.1769\n",
            "Epoch 10/10\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 51ms/step - accuracy: 0.9392 - loss: 0.1658\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9072 - loss: 0.2615\n",
            "Test accuracy: 0.9099000096321106\n"
          ]
        }
      ]
    }
  ]
}