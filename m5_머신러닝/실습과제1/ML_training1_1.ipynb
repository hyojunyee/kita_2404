{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG586pCWzZ19KszvmhiCHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyojunyee/kita_2404/blob/main/m5_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/Task/ML_training1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline 연습"
      ],
      "metadata": {
        "id": "VSjHTyQ0Pi7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "# 데이터 로딩\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 파이프라인 구성\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svd', TruncatedSVD(n_components=2)),\n",
        "    ('lr', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'svd__n_components': [2, 5, 10],\n",
        "    'lr__C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용한 하이퍼 파라미터 튜닝\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 하이퍼 파라미터 출력\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "\n",
        "# 평가 사용자 함수 정의\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # 예측 수행\n",
        "    y_pred = model.predict(X_test)\n",
        "    # 정확도 계산\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # 분류 보고서 생성\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    # ROC AUC 계산\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
        "    print(f\"Classification Report:\\n{report}\\n\")\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# 최적의 모델로 평가\n",
        "evaluate_model(grid_search, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyjaK1RePhfU",
        "outputId": "dbaadcd5-5b59-4b0a-8200-c2765571423a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'lr__C': 1, 'svd__n_components': 10}\n",
            "Test Accuracy: 0.9825\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98        43\n",
            "           1       0.99      0.99      0.99        71\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.98      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n",
            "\n",
            "ROC AUC: 0.9813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "California_housing 데이터셋으로 아래사항을 참조하여 주택가격을 예측하는 회귀모델을 개발하세요.\n",
        "- 전체 회귀모델을 적용\n",
        "- 각 모델별 최적 하이퍼파라미터 - GridSearchCV 활용\n",
        "- 평가지수 MSE 기준으로 가장 성능이 좋은 모델과 파라미터를 적용하여 평가 결과를 출력"
      ],
      "metadata": {
        "id": "Sor5BAvsXS7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터 로딩\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 데이터 프레임으로 변환\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Rgc4eTTvK55Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 모델 리스트\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge': Ridge(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'Random Forest': RandomForestRegressor(),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(),\n",
        "    'XGBoost': XGBRegressor(),\n",
        "    'LightGBM': LGBMRegressor(),\n",
        "    'SVR': SVR()\n",
        "}\n",
        "\n",
        "# 모델별 파이프라인 생성\n",
        "pipelines = {name: Pipeline([\n",
        "    ('scaler', StandardScaler()),  # 데이터 스케일링\n",
        "    ('model', model)               # 회귀 모델\n",
        "]) for name, model in models.items()}"
      ],
      "metadata": {
        "id": "8KCLKP2iTfgG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd03b34-7318-4fc0-a019-c89611a6acd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 파이프라인별 그리드서치 함수 정의\n",
        "def perform_grid_search(pipeline, param_grid, X_train, y_train, cv_folds=5):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv_folds, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "\n",
        "    return best_model, best_params, best_score"
      ],
      "metadata": {
        "id": "8sFfu_A-T9MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델별 하이퍼파라미터 그리드\n",
        "param_grids = {\n",
        "    'Linear Regression': {},\n",
        "    'Ridge': {'model__alpha': [0.1, 1, 10, 100]},\n",
        "    'Lasso': {'model__alpha': [0.1, 1, 10, 100]},\n",
        "    'ElasticNet': {'model__alpha': [0.1, 1, 10, 100],\n",
        "                   'model__l1_ratio': [0.1, 0.5, 0.9]},\n",
        "    'Random Forest': {\n",
        "        'model__n_estimators': [100, 200, 500],\n",
        "        'model__max_depth': [None, 10, 20, 30, 50],\n",
        "        'model__min_samples_leaf': [1, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model__n_estimators': [100, 200, 500],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model__n_estimators': [100, 200, 500],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model__n_estimators': [100, 200, 500],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2]\n",
        "    },\n",
        "    'SVR': {\n",
        "        'model__C': [0.1, 1, 10],\n",
        "        'model__epsilon': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "UvuTe78nVEJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 모델별로 그리드 서치를 수행\n",
        "results = {}\n",
        "for name, pipeline in pipelines.items():\n",
        "    print(f\"Performing Grid Search for {name}...\")\n",
        "    param_grid = param_grids.get(name, {})\n",
        "    best_model, best_params, best_score = perform_grid_search(pipeline, param_grid, X_train, y_train)\n",
        "    results[name] = {\n",
        "        'Best Model': best_model,\n",
        "        'Best Params': best_params,\n",
        "        'Best Score': best_score\n",
        "    }\n",
        "\n",
        "# 결과 출력\n",
        "for model_name, result in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Best Parameters: {result['Best Params']}\")\n",
        "    print(f\"Best Score (MSE): {result['Best Score']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-3hpBkpPefz",
        "outputId": "35bee97c-eaad-4829-a7a5-504c5cf91347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Grid Search for Linear Regression...\n",
            "Performing Grid Search for Ridge...\n",
            "Performing Grid Search for Lasso...\n",
            "Performing Grid Search for ElasticNet...\n",
            "Performing Grid Search for Random Forest...\n",
            "Performing Grid Search for Gradient Boosting...\n",
            "Performing Grid Search for XGBoost...\n",
            "Performing Grid Search for LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1838\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 2.071947\n",
            "Performing Grid Search for SVR...\n",
            "\n",
            "Model: Linear Regression\n",
            "Best Parameters: {}\n",
            "Best Score (MSE): -0.5192652011433679\n",
            "\n",
            "Model: Ridge\n",
            "Best Parameters: {'model__alpha': 0.1}\n",
            "Best Score (MSE): -0.5192652367434867\n",
            "\n",
            "Model: Lasso\n",
            "Best Parameters: {'model__alpha': 0.1}\n",
            "Best Score (MSE): -0.6720918682458477\n",
            "\n",
            "Model: ElasticNet\n",
            "Best Parameters: {'model__alpha': 0.1, 'model__l1_ratio': 0.1}\n",
            "Best Score (MSE): -0.579933805332222\n",
            "\n",
            "Model: Random Forest\n",
            "Best Parameters: {'model__max_depth': 50, 'model__min_samples_leaf': 1, 'model__n_estimators': 500}\n",
            "Best Score (MSE): -0.25865508780860863\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Best Parameters: {'model__learning_rate': 0.2, 'model__n_estimators': 500}\n",
            "Best Score (MSE): -0.2272719340913103\n",
            "\n",
            "Model: XGBoost\n",
            "Best Parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 500}\n",
            "Best Score (MSE): -0.20897054248310373\n",
            "\n",
            "Model: LightGBM\n",
            "Best Parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 500}\n",
            "Best Score (MSE): -0.2071575971223607\n",
            "\n",
            "Model: SVR\n",
            "Best Parameters: {'model__C': 10, 'model__epsilon': 0.2}\n",
            "Best Score (MSE): -0.3187104296797953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- LightGBM\n",
        "\n",
        "    - Hyperparameters: {'model__learning_rate': 0.1, 'model__n_estimators': 500}"
      ],
      "metadata": {
        "id": "w87qLCh6oQ_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "# import numpy as np\n",
        "\n",
        "# # 사용할 모델 리스트\n",
        "# selected_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']\n",
        "\n",
        "# # 각 모델에 대해 크로스 밸리데이션 수행\n",
        "# cv_results = {}\n",
        "# for name in selected_models:\n",
        "#     pipeline = pipelines[name]\n",
        "#     print(f\"Performing Cross Val Score for {name}...\")\n",
        "#     scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "#     mse_scores = -scores\n",
        "#     avg_mse = np.mean(mse_scores)\n",
        "\n",
        "#     cv_results[name] = {\n",
        "#         'Average MSE': avg_mse,\n",
        "#         'Scores': mse_scores\n",
        "#     }\n",
        "\n",
        "# # 결과 출력\n",
        "# for model_name, result in cv_results.items():\n",
        "#     print(f\"\\nModel: {model_name}\")\n",
        "#     print(f\"Average MSE: {result['Average MSE']:.4f}\")\n",
        "#     print(f\"Individual Fold MSEs: {result['Scores']}\")"
      ],
      "metadata": {
        "id": "joOCMBPX8TiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "427c98f4-a64d-42ac-af9d-2ee8afb4787d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing Cross Val Score for Random Forest...\n",
            "Performing Cross Val Score for Gradient Boosting...\n",
            "Performing Cross Val Score for XGBoost...\n",
            "Performing Cross Val Score for LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001644 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1838\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 2.164930\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002277 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1838\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 2.034871\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1838\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 1.995290\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001348 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1838\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 2.127029\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001327 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1837\n",
            "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
            "[LightGBM] [Info] Start training from score 2.020670\n",
            "\n",
            "Model: Random Forest\n",
            "Average MSE: 0.4317\n",
            "Individual Fold MSEs: [0.5280378  0.34447931 0.37573808 0.44721691 0.46303939]\n",
            "\n",
            "Model: Gradient Boosting\n",
            "Average MSE: 0.4125\n",
            "Individual Fold MSEs: [0.42707306 0.3526041  0.40566397 0.41036482 0.46656392]\n",
            "\n",
            "Model: XGBoost\n",
            "Average MSE: 0.4383\n",
            "Individual Fold MSEs: [0.45527649 0.34084391 0.38503177 0.49006184 0.52024501]\n",
            "\n",
            "Model: LightGBM\n",
            "Average MSE: 0.3889\n",
            "Individual Fold MSEs: [0.40832509 0.33261795 0.36352077 0.37215727 0.46788884]\n"
          ]
        }
      ]
    }
  ]
}
