선형 변환과 비선형 활성화 함수의 조합
신경망의 기본 구조와 작동 원리

1. 선형 변환 (가중치 행렬)
선형 변환은 입력 데이터를 선형적으로 변환하는 과정입니다. 이는 가중치 행렬과 입력 벡터의 곱셈으로 이루어집니다.

입력 벡터 (
𝑥
x): 신경망에 입력되는 데이터. 예를 들어, 이미지의 픽셀 값 또는 텍스트 데이터의 특징 벡터입니다.
가중치 행렬 (
𝑊
W): 신경망이 학습하는 값으로, 입력 데이터를 변환하는 역할을 합니다.
편향 벡터 (
𝑏
b): 추가적인 조정 값으로, 입력 데이터가 모든 가중치에 영향을 받지 않도록 합니다.
선형 변환은 다음과 같이 표현할 수 있습니다:
𝑧
=
𝑊
𝑥
+
𝑏
z=Wx+b

여기서 
𝑧
z는 선형 변환의 결과입니다.

2. 비선형 활성화 함수
비선형 활성화 함수는 선형 변환의 결과에 비선형성을 부여하는 함수입니다. 이를 통해 신경망이 복잡한 패턴을 학습할 수 있습니다.

일반적인 활성화 함수에는 다음과 같은 것들이 있습니다:

ReLU (Rectified Linear Unit):
ReLU(z)=max(0,z)

ReLU(z)=max(0,z)
시그모이드 (Sigmoid):
𝜎
(
𝑧
)
=
1
1
+
𝑒
−
𝑧
σ(z)= 
1+e 
−z
 
1
​
 
하이퍼볼릭 탄젠트 (Tanh):
tanh
⁡
(
𝑧
)
=
𝑒
𝑧
−
𝑒
−
𝑧
𝑒
𝑧
+
𝑒
−
𝑧
tanh(z)= 
e 
z
 +e 
−z
 
e 
z
 −e 
−z
 
​
 
활성화 함수의 목적은 선형 변환의 결과를 비선형적으로 변환하여 신경망이 비선형 관계를 학습할 수 있도록 하는 것입니다.

3. 조합의 이해
신경망은 여러 층으로 구성되며, 각 층에서 선형 변환과 비선형 활성화 함수가 반복적으로 적용됩니다.

입력 데이터: 
𝑥
x
첫 번째 층:
𝑧
1
=
𝑊
1
𝑥
+
𝑏
1
z 
1
​
 =W 
1
​
 x+b 
1
​
 
𝑎
1
=
ReLU
(
𝑧
1
)
a 
1
​
 =ReLU(z 
1
​
 )
두 번째 층:
𝑧
2
=
𝑊
2
𝑎
1
+
𝑏
2
z 
2
​
 =W 
2
​
 a 
1
​
 +b 
2
​
 
𝑎
2
=
ReLU
(
𝑧
2
)
a 
2
​
 =ReLU(z 
2
​
 )
출력 층:
𝑧
3
=
𝑊
3
𝑎
2
+
𝑏
3
z 
3
​
 =W 
3
​
 a 
2
​
 +b 
3
​
 
𝑎
3
=
Softmax
(
𝑧
3
)
a 
3
​
 =Softmax(z 
3
​
 ) (분류 문제의 경우)
각 층에서 입력 벡터가 선형 변환을 거친 후 비선형 활성화 함수를 통해 변환됩니다. 이를 통해 신경망은 복잡한 함수의 근사값을 학습할 수 있습니다.

요약
선형 변환: 입력 데이터를 가중치 행렬과 곱하여 변환하는 과정입니다.
비선형 활성화 함수: 선형 변환의 결과에 비선형성을 부여하여 신경망이 복잡한 패턴을 학습할 수 있도록 합니다.
조합: 여러 층에서 선형 변환과 비선형 활성화 함수가 반복적으로 적용되어 신경망이 데이터의 비선형 관계를 학습합니다.
